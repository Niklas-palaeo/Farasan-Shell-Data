---
title: "Farasan Shell Database"
author: "Niklas Hausmann"
date: "01/07/2020"
output: html_document
---
#Introduction
This file explains the process behind the `Farasan Database.R` file as well as the .csv files for the weight data by species ('Species.csv') and aperture sizes by shell ('Apertures.csv') that are referred to in the other documents, which carry out the actual analysis and illustration. Is is not essential to understand these steps, as none of the data are being changed, but it can help to understand how and why the data are structured as they are.
This document will also not compile the entire dataset, so going through the steps will not produce the final database.
You will have to load the .R or .csv files separately, which will be explained in the other documents.

## Packages used
Some of the steps below require certain packages to be installed/activated. These are

```{r echo=TRUE, message=FALSE}
library(magrittr)
library(purrr)
library(here)
```


## Importing data from .csv files
All .csv files are located in the folder *Raw Exports*. We ar eusing the 'here()' package which will help with the file paths. All files are in the Raw exports folder and you won't have to navigate much, if you use the Rproject file 'Farasan.RPoj'

The steps in the file are all essentially the same process over and over again, and should really be a loop or a function. It just happened to be difficult to make any process work for each .csv file and sometimes special solutions needed to be found, because the .csv files had inconsistent formatting. So now these files are all imported individually. That said, this import will happen in the background without you noticing, so it shouldn't be a problem.  
The way the main list of data is structured is with the sites on the first level, the bulk sample number on the second level and the samples species as well as basic stats on the third level. That said, there are variation to this main list, which is further described below.

|**Level**|**Data**|
|---|---|
|1st|Bay Area|
|2nd|Site|
|3rd|Bulk sample|
|4th|Species name or basic stats|

Below is an example using the site of JW1864 located in the Janaba West site cluster. Most sites are imported exactly as is described below, but for interested readers it might be worth looking at the steps for other sites as well.
If you run the code below, you will get a list of all bulk samples in JW1864 with measurements in grams and the corresponding aperture sizes in mm. The output below the code is the first bulk sample of the site.

```{r Example data import}
#Data from Janaba West
#JW1864####
    #This is the species data
JW1864 <- read.csv(here("data/Raw Exports/JW1864.csv"))
    #This is the aperture data
ApsJW1864<- read.csv(here("data/Raw Exports/JW1864-Apertures.csv"))%>%
  na.omit()
    #Species structured as a list
JW1864 <- nrow(JW1864)%>%
  seq(.)%>%
  split(JW1864,.)%>%
  setNames(.,JW1864$Sample.No)
JW1864 <- modify_depth(JW1864,1,~ discard(.x,is.na))#Remove NA columns
    #Apertures structured into lists
ApGroupsJW1864<- split( ApsJW1864 ,f = ApsJW1864$Sample.No)
ApGroupsJW1864 <- modify_depth(ApGroupsJW1864,1,~ list_modify(.x,"Sample.No" = NULL)) #remove sample.No element
    #Add Apertures to list of site
Sfas <- "Strombus.fasciatus..............Born.1778..Aperture" #make code more readable when dealing with S. fasciatus apertures
for (i in 1:length(JW1864))  {JW1864[[i]][[Sfas]] <- c(ApGroupsJW1864[[i]])}
rm(ApGroupsJW1864, ApsJW1864,i)
JW1864[1]
```


After the lists for each site have been build, they are combined in several different ways, depending on what was most suitable for the following analyses. First the site-lists are combined into one bay area (e.g. Janaba West),
```{r Making lists by bay area, eval=FALSE, include=TRUE}

TPs <- read.csv(here("data/Raw Exports/JWTP.csv"))

Janaba_West <- 
  list(#JW0001 to JW0021 are TestPits, which had already been combined into one list
    "JW0001" = TPs[[1]],
    "JW0002" = TPs[[2]],
    "JW0003" = TPs[[3]],
    "JW0004" = TPs[[4]],
    "JW0005" = TPs[[5]],
    "JW0006" = TPs[[6]],
    "JW0007" = TPs[[7]],
    "JW0008" = TPs[[8]],
    "JW0009" = TPs[[9]],
    "JW0010" = TPs[[10]],
    "JW0011" = TPs[[11]],
    "JW0012" = TPs[[12]],
    "JW0013" = TPs[[13]],
    "JW0014" = TPs[[14]],
    "JW0015" = TPs[[15]],
    "JW0016" = TPs[[16]],
    "JW0017" = TPs[[17]],
    "JW0018" = TPs[[18]],
    "JW0019" = TPs[[19]],
    "JW0021" = TPs[[20]],
    #Here is where the full columns at each site start and where you will get an error because we didn't import every site here, only JW1864
    "JW1864" = JW1864,
    "JW1705" = JW1705,
    "JW1727_A" = JW1727_A,
    "JW1727_B" = JW1727_B,#JW1727 has two columns
    "JW1807" = JW1807,
    "JW2298" = JW2298,
    "JW3120" = JW3120,
    "JW5697" = JW5697
  )
```

and subsequently into the all-encompassing list **Areas**, that is described in the table above.

```{r eval=FALSE, include=TRUE}
Areas<- list("Khur_Maadi"=Khur_Maadi,"Janaba_West"=Janaba_West,"Janaba_East"=Janaba_East)

```

Other lists include all sites without grouping by area:

```{r eval=FALSE, include=TRUE}
Sites <- c(Khur_Maadi,Janaba_West,Janaba_East)
```

Or include only the information about species weight and removing any other information:

```{r eval=FALSE, include=TRUE}
Species <- c(Khur_Maadi,Janaba_West,Janaba_East)
toremove <- "MNI|Saudi|Shell.Only.Weight|MNS|Site.Reference|Stone|Date|Operculum|Stuff|Barnacle|X|Context|Depth|Nother|Bone|Charcoal|Chiton|Pearl|Calcite|still.to.be|Possibly|Column|Unidentified|Fragments|SHELLWEIGHT|Length|Width|Height|Aperture|Upper|Lower|Left|Right|Coral|Notes|Sample.Number|Sample.No|Layer|Position|Spit|Year.Ref|Minimum.Number.of.Species..MNS.|Burnt.Shell|Crab|Fishbone|Worm|Breccia|Sorted|Urchin|Residue|York|Total"
Species <- modify_depth(Species,2,~ discard(.x,grepl(toremove,names(.x))))
```

Or include only the aperture values:

```{r eval=FALSE, include=TRUE}
Apertures <- c(Khur_Maadi,Janaba_West,Janaba_East)
Apertures <- modify_depth(Apertures,2,~ keep(.x,grepl(Sfas,names(.x))))
```

## Calling these datasets in other scripts
At the end of the `Farasan Database.R` file, you will have four lists remaining: `Areas`, `Sites`, `Species`, and `Apertures`.

```{r eval=FALSE, include=TRUE}
rm(list=setdiff(ls(), c("Areas","Sites","Species","Apertures")))
```

This means that whenever you source the file via another R-Script (e.g. `Basic Shell Information`), you will receive these 4 lists. 

```{r eval=FALSE, include=TRUE}
source('Farasan Database.R')
```


However, due to the nature of the cleaning step at the end of the document, it will remove any other variable in the current environment. This is why the database is sourced at the beginning of each script, when no variables exist yet.  
**Note**: It can happen that some packages prevent the import of data when sourcing `Farasan Database.R`. This is also why the dataset is sourced prior to activating the packages in the other R scripts. If you run into problem with getting the data imported. Try to restart RStudio and make importing the data the first step.

Or simply use the csv files 'Species.csv' and 'Apertures.csv'.
